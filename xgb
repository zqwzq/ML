data <- read.xlsx("E:\\研究生论文\\真正的大论文\\建模\\data筛选.xlsx")
set.seed(500)
index <- createDataPartition(data$SOC,p = 0.7)
test_data <- data[-index$Resample1,]
train_data <- data[index$Resample1,]
xgbcontrol <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(nrounds = seq(100,1100,200),
                         max_depth = seq(1,10,3) ,##(0,10)
                         eta = 0.3,
                         gamma = seq(0,0.5,0.2),
                         colsample_bytree = seq(0.5,1,0.2),##(0.5,1)
                         min_child_weight =seq(1,10,3),
                         subsample = seq(0.1,1,0.1))##(0,1)
xgbreg <- train(SOC ~., data = train_data, method = "xgbTree",
                trControl = xgbcontrol,tuneGrid = tune_grid,
                verbose = FALSE,
                verbosity = 0
                )
print(xgbreg$bestTune)
#循环回归
XGB_func <- function(seed){
  set.seed(seed)
xgbcontrol <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(nrounds = xgbreg$bestTune$nrounds,
                         max_depth = xgbreg$bestTune$max_depth,
                         eta = xgbreg$bestTune$eta,
                         gamma = xgbreg$bestTune$gamma,
                         colsample_bytree = xgbreg$bestTune$colsample_bytree,
                         min_child_weight =xgbreg$bestTune$min_child_weight,
                         subsample = xgbreg$bestTune$subsample)
xgbreg <- train(SOC ~., data = train_data, method = "xgbTree",
                trControl = xgbcontrol,tuneGrid = tune_grid,
                verbose = FALSE,
                verbosity = 0
                )
yp <- predict(xgbreg,newdata = test_data)
y <- test_data$SOC
Sregg=sum((yp-mean(y))^2)
Stott=sum((mean(y)-y)^2)
Sress=sum((yp-y)^2)
#test_rsquared=Sregg/Stott
#test_rsquared = R2(y,yp)
test_rsquared=1-Sress/Stott
test_mae= sum(abs(yp-y))/ 32
test_rmse = sqrt(  (sum((yp-y)^2))  /32  )

train_rpd <- sd(train_data[,1])/xgbreg$results$RMSE
train_nrmse <- xgbreg$results$RMSE/(max(train_data[,1])-min(train_data[,1]))
test_rpd <- sd(test_data[,1])/test_rmse
test_nrmse <- test_rmse/(max(test_data[,1])-min(test_data[,1]))
  return(c(xgbreg$results$RMSE,xgbreg$results$Rsquared,xgbreg$results$MAE,train_rpd,train_nrmse,test_rmse,test_rsquared,test_mae,test_rpd,test_nrmse))
}
seed <- seq(10000,11000,10)
n <- length(seed)
zhibiao_list <- lapply(seed, XGB_func)
zhibiao <- matrix(as.numeric(unlist(zhibiao_list)),nrow = n, ncol = 10, byrow = TRUE)
sprintf("Train_XGB_RMSE为：%f",mean(zhibiao[,1]))
sprintf("Train_XGB_R2为：%f",mean(zhibiao[,2]))
sprintf("Train_XGB_MAE为：%f",mean(zhibiao[,3]))
sprintf("Train_XGB_RPD为：%f",mean(zhibiao[,4]))
sprintf("Train_XGB_nRMSE为：%f",mean(zhibiao[,5]))
sprintf("Test_XGB_RMSE为：%f",mean(zhibiao[,6]))
sprintf("Test_XGB_R2为：%f",mean(zhibiao[,7]))
sprintf("Test_XGB_MAE为：%f",mean(zhibiao[,8]))
sprintf("Test_XGB_RPD为：%f",mean(zhibiao[,9]))
sprintf("Test_XGB_nRMSE为：%f",mean(zhibiao[,10]))
xgbreg$bestTune

##测试集预测 散点图

data <- read.xlsx("E:\\研究生论文\\大论文\\数据\\建模\\choosed.xlsx")
set.seed(200)
index <- createDataPartition(data$SOC,p = 0.7)
test_data <- data[-index$Resample1,]
train_data <- data[index$Resample1,]
XGB_func <- function(seed){
  set.seed(seed)
 xgbcontrol <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(nrounds = xgbreg$bestTune$nrounds,
                         max_depth = xgbreg$bestTune$max_depth,
                         eta = xgbreg$bestTune$eta,
                         gamma = xgbreg$bestTune$gamma,
                         colsample_bytree = xgbreg$bestTune$colsample_bytree,
                         min_child_weight =xgbreg$bestTune$min_child_weight,
                         subsample = xgbreg$bestTune$subsample)
xgbreg <- train(SOC ~., data = train_data, method = "xgbTree",
                trControl = xgbcontrol,tuneGrid = tune_grid,
                verbose = FALSE,
                verbosity = 0
                )
yp <- predict(xgbreg,newdata = test_data)
yp <- as.data.frame(yp)
return(yp)
}
seed <- seq(10000,11000,10)
n <- length(seed)
ypall <- lapply(seed, XGB_func)
total <- 0
for (i in 1:n) {
  total <-total+ypall[[i]]
}
ypall_mean <- total/n

##反演
data <- read.xlsx("E:\\研究生论文\\真正的大论文\\建模\\data筛选.xlsx")
fanyan_data <- read.xlsx("E:\\研究生论文\\真正的大论文\\remotesensingdata\\POINT\\200W-.xlsx")
set.seed(500)
index <- createDataPartition(data$SOC,p = 0.7)
test_data <- data[-index$Resample1,]
train_data <- data[index$Resample1,]
XGB_func <- function(seed){
  set.seed(seed)
 xgbcontrol <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(nrounds = xgbreg$bestTune$nrounds,
                         max_depth = xgbreg$bestTune$max_depth,
                         eta = xgbreg$bestTune$eta,
                         gamma = xgbreg$bestTune$gamma,
                         colsample_bytree = xgbreg$bestTune$colsample_bytree,
                         min_child_weight =xgbreg$bestTune$min_child_weight,
                         subsample = xgbreg$bestTune$subsample)
xgbreg <- train(SOC ~., data = train_data, method = "xgbTree",
                trControl = xgbcontrol,tuneGrid = tune_grid,
                verbose = FALSE,
                verbosity = 0
                )
fanyan_yp <- predict(xgbreg,newdata = fanyan_data)
fanyan_yp <- as.data.frame(fanyan_yp)
return(fanyan_yp)
}
seed <- seq(10000,11000,10)
n <- length(seed)
fanyan_ypall <- lapply(seed, XGB_func)
total <- 0
for (i in 1:n) {
  total <-total+fanyan_ypall[[i]]
}
fanyan_ypall_mean <- total/n
write.table(fanyan_ypall_mean,file = "E:\\研究生论文\\真正的大论文\\remotesensingdata\\FANYAN\\XGB_200w-_prediction_mean.txt",row.names = F,col.names = F)
a <- nrow(fanyan_data)
danxiangyuan <- rep(0,100)
quantu <- rep(0,a)
for(k in 1:a){
  for (j in 1:100){
    danxiangyuan[j] <- fanyan_ypall[[j]][["fanyan_yp"]][k]
  }
  quantu[k] <- sd(danxiangyuan)
}
write.table(quantu,file = "E:\\研究生论文\\真正的大论文\\remotesensingdata\\FANYAN\\XGB_200w-_prediction_SD.txt",row.names = F,col.names = F)


##重要性
data <- read.xlsx("E:\\研究生论文\\大论文\\数据\\建模\\choosed.xlsx")
set.seed(200)
index <- createDataPartition(data$SOC,p = 0.7)
test_data <- data[-index$Resample1,]
train_data <- data[index$Resample1,]
XGB_func <- function(seed){
set.seed(seed)
xgbcontrol <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(nrounds = xgbreg$bestTune$nrounds,
                         max_depth = xgbreg$bestTune$max_depth,
                         eta = xgbreg$bestTune$eta,
                         gamma = xgbreg$bestTune$gamma,
                         colsample_bytree = xgbreg$bestTune$colsample_bytree,
                         min_child_weight =xgbreg$bestTune$min_child_weight,
                         subsample = xgbreg$bestTune$subsample)
xgbreg <- train(SOC ~., data = train_data, method = "xgbTree",
                trControl = xgbcontrol,tuneGrid = tune_grid,
                verbose = FALSE,
                verbosity = 0
                )
zhongyaoxing <- varImp(xgbreg)[["importance"]]
  return(zhongyaoxing)
}
seed <- seq(1,100,1)
n <- length(seed)
zhibiao_list <- lapply(seed, XGB_func)
total <- 0
for (i in 1:n) {
  total <-total+zhibiao_list[[i]]
}
zhongyaoxing <- total/length(seed)
