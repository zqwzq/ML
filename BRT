##################################################################
## The data is read and randomly divided 7:3 into training sets and test sets
## Reading data. where "data.xlsx" is data storage
data <- read.xlsx("data.xlsx")
## Select the seed number to divide the data set. Note that the seed number of the partition data set cannot be changed.
set.seed(500)
## 70% as the training set, 30% as the test set
index <- createDataPartition(data$SOC,p = 0.7)
test_data <- data[-index$Resample1,]
train_data <- data[index$Resample1,]
##################################################################
## The mesh is defined and the optimal parameters are adjusted by grid search method
## Set 10 fold cross validation
brtcontrol <- trainControl(method = "cv",number = 10, search="grid")
## Set adjustment grid and spacing.
tune_grid <- expand.grid(n.trees =seq(50,2000,50),
                        interaction.depth = seq(1, 10, 1),
                        shrinkage = seq(0.1, 1, 0.01),
                        n.minobsinnode = seq(0.1, 1, 0.1))
## Different parameters are used for machine learning modeling 
brtreg <- train(SOC ~., data = train_data, method = "gbm",
               trControl = brtcontrol, tuneGrid = tune_grid,
               verbose = FALSE
              )
## Print the optimal parameter combination
print(brtreg$results)
##################################################################
##Use different seed numbers to build machine learning models, and output precision indicators of training sets and test sets. Refer to Step C in README for details.
## Define a machine learning function for a multi-seed loop
BRT_func <- function(seed){
## Model with different seed numbers
set.seed(seed)
## Set 10 fold cross validation
brtcontrol <- trainControl(method = "cv",number = 10, search="grid")
## The optimal parameters are used as the model parameters for machine learning modeling
tune_grid <- expand.grid(n.trees = brtreg$bestTune$n.trees,
                        interaction.depth = brtreg$bestTune$interaction.depth,
                        shrinkage = brtreg$bestTune$shrinkage,
                        n.minobsinnode = brtreg$bestTune$n.minobsinnode)
brtreg <- train(SOC ~., data = train_data, method = "gbm",
               trControl = brtcontrol,tuneGrid = tune_grid,
               verbose = FALSE)
## The established machine learning model is used to predict the test set data
yp <- predict(brtreg,newdata = test_data)
## Extract the measured data of the test set
y <- test_data$SOC
## Calculate the accuracy index R2 of the test set
Sregg=sum((yp-mean(y))^2)
Stott=sum((mean(y)-y)^2)
Sress=sum((yp-y)^2)
test_rsquared=Sregg/Stott
## Calculate the accuracy index MAE of the test set
test_mae= sum(abs(yp-y))/ 32
## Calculate the accuracy index RMSE of the test set
test_rmse = sqrt(  (sum((yp-y)^2))  /32 )
## Calculate the accuracy index RPD of the test set
test_rpd <- sd(test_data[,1])/test_rmse
## Calculate the accuracy index RPD of the train set
train_rpd <- sd(train_data$SOC)/brtreg$results$RMSE
## Extract the nRMSE of the model
train_nrmse <- brtreg$results$RMSE/(max(train_data[,1])-min(train_data[,1]))
## Calculate the accuracy index nRMSE of the test set
test_nrmse <- test_rmse/(max(test_data[,1])-min(test_data[,1]))
## Returns each precision indicator of the model
return(c(brtreg$results$RMSE,brtreg$results$Rsquared,brtreg$results$MAE,train_rpd,train_nrmse,test_rmse,test_rsquared,test_mae,test_rpd,test_nrmse))
}
## Sets the number of seeds to use for machine learning modeling
seed <- seq(10000,11000,10)    
## The number of seed number is extracted and used to calculate the average value of precision index
n <- length(seed)
## The set number of seeds is applied for circular machine learning modeling
zhibiao_list <- lapply(seed, BRT_func)
## Define a matrix for storing precision indicators and calculating average values
zhibiao <- matrix(as.numeric(unique(unlist(zhibiao_list))),nrow = n, ncol = 10, byrow = TRUE)
## Output the RMSE of the training set
sprintf("Train_BRT_RMSE为：%f",mean(zhibiao[,1]))
## Output the R2 of the training set
sprintf("Train_BRT_R2为：%f",mean(zhibiao[,2]))
## Output the MAE of the training set
sprintf("Train_BRT_MAE为：%f",mean(zhibiao[,3]))
## Output the RPD of the training set
sprintf("Train_BRT_RPD为：%f",mean(zhibiao[,4]))
## Output the nRMSE of the training set
sprintf("Train_BRT_nRMSE为：%f",mean(zhibiao[,5]))
## Output the RMSE of the test set
sprintf("Test_BRT_RMSE为：%f",mean(zhibiao[,6]))
## Output the R2 of the test set
sprintf("Test_BRT_R2为：%f",mean(zhibiao[,7]))
## Output the MAE of the test set
sprintf("Test_BRT_MAE为：%f",mean(zhibiao[,8]))
## Output the RPD of the test set
sprintf("Test_BRT_RPD为：%f",mean(zhibiao[,9]))
## Output the RPD of the test set
sprintf("Test_BRT_nRMSE为：%f",mean(zhibiao[,10]))


##################################################################
##Use different seed numbers to build machine learning models, and output dependent variable of training sets and test sets. Refer to Step D in README for details.
## Define a machine learning function for a multi-seed loop
BRT_func <- function(seed){
## Model with different seed numbers
set.seed(seed)
## Set 10 fold cross validation
brtcontrol <- trainControl(method = "cv",number = 10, search="grid")
## The optimal parameters are used as the model parameters for machine learning modeling
tune_grid <- expand.grid(n.trees = brtreg$bestTune$n.trees,
                        interaction.depth = brtreg$bestTune$interaction.depth,
                        shrinkage = brtreg$bestTune$shrinkage,
                        n.minobsinnode = brtreg$bestTune$n.minobsinnode)
brtreg <- train(SOC ~., data = train_data, method = "gbm",
               trControl = brtcontrol,tuneGrid = tune_grid,
               verbose = FALSE)
## The established machine learning model is used to predict the test set data
yp <- predict(brtreg,newdata = test_data)
##  Convert the predicted test set dependent variable to data frame format
yp <- as.data.frame(yp)
return(yp)
}
## Sets the number of seeds to use for machine learning modeling. Consistent with modeling.
seed <- seq(10000,11000,10) 
## The number of seed number is extracted and used to calculate the average value of precision index
n <- length(seed)  
## The set number of seeds is applied for circular machine learning modeling
ypall <- lapply(seed, RF_func)    
##  Initializes a variable to hold the predicted test set dependent variable
total <- 0   
##  Add up the results of each run
for (i in 1:n) {
  total <-total+ypall[[i]]
}    
## Gets the average of multiple model predictions
ypall_mean <- total/n


##################################################################
##Use different seed numbers to build machine learning models, and output dependent variable of training sets and test sets. Refer to Step D in README for details.
## Define a machine learning function for a multi-seed loop
BRT_func <- function(seed){
## Model with different seed numbers
set.seed(seed)
## Set 10 fold cross validation
control <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(n.trees = brtreg$bestTune$n.trees,
                        interaction.depth = brtreg$bestTune$interaction.depth,
                        shrinkage = brtreg$bestTune$shrinkage,
                        n.minobsinnode = brtreg$bestTune$n.minobsinnode)
## The optimal parameters are used as the model parameters for machine learning modeling
brtreg <- train(SOC ~., data = train_data, method = "gbm",
               trControl = brtcontrol,tuneGrid = tune_grid,
               verbose = FALSE)
## The independent variable value of each pixel is brought into the model to predict the SOC value of each pixel
fanyan_yp <- predict(brtreg,newdata = fanyan_data)
##  Convert the SOC value to data frame format
fanyan_yp <- as.data.frame(fanyan_yp)
return(fanyan_yp)
}
## Sets the number of seeds to use for machine learning modeling. Consistent with modeling.
seed <- seq(10000,11000,10)
## The number of seed number is extracted and used to calculate the average value of precision index
n <- length(seed)
## The set number of seeds is applied for circular machine learning modeling
fanyan_ypall <- lapply(seed, BRT_func)
##  Initializes a variable to hold the predicted SOC value
total <- 0
##  Add up the results of each run
for (i in 1:n) {
  total <-total+fanyan_ypall[[i]]
}
## Gets the average of multiple model predictions
fanyan_ypall_mean <- total/n
## Write out the inversion results
write.table(fanyan_ypall_mean,file = "FANYAN.txt",row.names = F,col.names = F)


##################################################################
##Use different seed numbers to build machine learning models, and output dependent variable of training sets and test sets. Refer to Step D in README for details.
## Define a machine learning function for a multi-seed loop
BRT_func <- function(seed){
## Model with different seed numbers
set.seed(seed)
## Set 10 fold cross validation
brtcontrol <- trainControl(method = "cv",number = 10, search="grid")
tune_grid <- expand.grid(n.trees = brtreg$bestTune$n.trees,
                        interaction.depth = brtreg$bestTune$interaction.depth,
                        shrinkage = brtreg$bestTune$shrinkage,
                        n.minobsinnode = brtreg$bestTune$n.minobsinnode)
## The optimal parameters are used as the model parameters for machine learning modeling
brtreg <- train(SOC ~., data = train_data, method = "gbm",
               trControl = brtcontrol,tuneGrid = tune_grid,
               verbose = FALSE)
## Get the importance of each variable from the model
zhongyaoxing <- summary(brtreg)
  return(zhongyaoxing)
}
## Sets the number of seeds to use for machine learning modeling. Consistent with modeling.
seed <- seq(10000,11000,10)
## The number of seed number is extracted and used to calculate the average value of precision index
n <- length(seed)
## The set number of seeds is applied for circular machine learning modeling
zhongyaoxing_list <- lapply(seed, BRT_func)
##  Initialize a variable to store the importance of each predictor
total <- 0
##  Add the importance of each variable for each loop. The BRT model computes importance by sorting the variable names first and then adding them in turn
for (i in 1:n) {
  a<- as.data.frame(zhongyaoxing_list[[i]])
  sorted_df <- a[order(row.names(a)), ]
  total <-total+sorted_df$rel.inf
}
##  The average value of variable importance is obtained by dividing the number of models
zhongyaoxing <- total/length(seed)
